#!/usr/bin/env python3
"""
æ›´æ–°å†å²æ•°æ®ä¸­çš„ GitHub Pages åœ°å€ä¸º raw.githubusercontent.com æ ¼å¼
"""

import json
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from main_extract_fast import _convert_github_pages_to_raw, canonicalize_url

def update_history_urls(filepath: str):
    """æ›´æ–°å†å²æ–‡ä»¶ä¸­çš„ GitHub Pages åœ°å€"""
    
    if not os.path.exists(filepath):
        print(f"å†å²æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return
    
    print(f"æ­£åœ¨æ›´æ–°å†å²æ–‡ä»¶: {filepath}")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å¤‡ä»½åŸå§‹æ•°æ®
    backup_path = filepath + '.url_update_backup'
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"å·²å¤‡ä»½åŸå§‹æ•°æ®åˆ°: {backup_path}")
    
    conversion_count = 0
    
    # æ›´æ–° seen åˆ—è¡¨
    if 'seen' in data:
        updated_seen = []
        for url in data['seen']:
            converted_url = canonicalize_url(url)
            if converted_url != url:
                conversion_count += 1
                print(f"[seenè½¬æ¢] {url} -> {converted_url}")
            updated_seen.append(converted_url)
        data['seen'] = updated_seen
    
    # æ›´æ–° links åˆ—è¡¨
    if 'links' in data:
        updated_links = []
        for url in data['links']:
            converted_url = canonicalize_url(url)
            if converted_url != url:
                conversion_count += 1
                print(f"[linksè½¬æ¢] {url} -> {converted_url}")
            updated_links.append(converted_url)
        data['links'] = updated_links
    
    # æ›´æ–° resource_keysï¼ˆè¿™ä¸ªéœ€è¦é‡æ–°æ„å»ºé”®ï¼‰
    if 'resource_keys' in data:
        updated_resource_keys = {}
        for url, meta in data['resource_keys'].items():
            converted_url = canonicalize_url(url)
            if converted_url != url:
                conversion_count += 1
                print(f"[resource_keysè½¬æ¢] {url} -> {converted_url}")
            updated_resource_keys[converted_url] = meta
        data['resource_keys'] = updated_resource_keys
    
    # æ›´æ–° fail è®°å½•
    if 'fail' in data:
        updated_fail = {}
        for url, count in data['fail'].items():
            converted_url = canonicalize_url(url)
            if converted_url != url:
                conversion_count += 1
                print(f"[failè½¬æ¢] {url} -> {converted_url}")
            updated_fail[converted_url] = count
        data['fail'] = updated_fail
    
    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… URL æ›´æ–°å®Œæˆï¼Œå…±è½¬æ¢äº† {conversion_count} ä¸ªåœ°å€")

def main():
    """ä¸»å‡½æ•°"""
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    history_path = os.path.join(project_root, 'data', 'history.json')
    
    print("ğŸ”¥ å¼€å§‹æ›´æ–° GitHub Pages åœ°å€...")
    
    # æ›´æ–°å†å²æ–‡ä»¶
    update_history_urls(history_path)
    
    print("\nâœ… æ›´æ–°å®Œæˆï¼")

if __name__ == "__main__":
    main()